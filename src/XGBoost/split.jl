
function getweight(gradient::LossGradient{T}, Î»::T) where {T<:AbstractFloat} 
    -gradient.âˆ‚ğ‘™ / (gradient.âˆ‚Â²ğ‘™ + Î»)
end

function getloss(âˆ‚ğ‘™::T, âˆ‚Â²ğ‘™::T, Î»::T) where {T<:AbstractFloat} 
    a::T = -0.5
    a * âˆ‚ğ‘™ * âˆ‚ğ‘™ / (âˆ‚Â²ğ‘™ + Î»)
end

function getloss(node::LeafNode{T}, Î»::T) where {T<:AbstractFloat} 
    âˆ‚ğ‘™ = node.gradient.âˆ‚ğ‘™
    âˆ‚Â²ğ‘™ = node.gradient.âˆ‚Â²ğ‘™
    getloss(âˆ‚ğ‘™, âˆ‚Â²ğ‘™, Î»)
end

function getloss(node::SplitNode{T}, Î»::T) where {T<:AbstractFloat} 
    node.loss
end

function getgain(node::LeafNode{T}) where {T<:AbstractFloat}
    zero(T)
end

function getgain(node::SplitNode{T}) where {T<:AbstractFloat}
    node.gain
end

function sumgradientslice!(âˆ‚ğ‘™sum0, âˆ‚Â²ğ‘™sum0, nodeids::Vector{<:Integer}, nodecansplit::Vector{Bool}, factor::AbstractFactor,
                           âˆ‚ğ‘™covariate::AbstractCovariate{T}, âˆ‚Â²ğ‘™covariate::AbstractCovariate{T}, fromobs::Integer, toobs::Integer, slicelength::Integer) where {T<:AbstractFloat}

    nodeslices = slice(nodeids, fromobs, toobs, slicelength)
    factorslices = slice(factor, fromobs, toobs, slicelength)
    âˆ‚ğ‘™slices = slice(âˆ‚ğ‘™covariate, fromobs, toobs, slicelength)
    âˆ‚Â²ğ‘™slices = slice(âˆ‚Â²ğ‘™covariate, fromobs, toobs, slicelength)
    zipslices = zip(nodeslices, factorslices, âˆ‚ğ‘™slices, âˆ‚Â²ğ‘™slices)
    fold((âˆ‚ğ‘™sum0, âˆ‚Â²ğ‘™sum0), zipslices) do gradsum, zipslice
        nodeslice, factorslice, âˆ‚ğ‘™slice, âˆ‚Â²ğ‘™slice = zipslice
        âˆ‚ğ‘™sum, âˆ‚Â²ğ‘™sum = gradsum
        @inbounds for i in 1:length(nodeslice)
            nodeid = nodeslice[i]
            if nodecansplit[nodeid]
                levelindex = factorslice[i] + 1
                âˆ‚ğ‘™sum[nodeid][levelindex] += âˆ‚ğ‘™slice[i]
                âˆ‚Â²ğ‘™sum[nodeid][levelindex] += âˆ‚Â²ğ‘™slice[i]
            end
        end
        (âˆ‚ğ‘™sum, âˆ‚Â²ğ‘™sum)
    end
end

function sumgradient(nodeids::Vector{<:Integer}, nodecansplit::Vector{Bool}, factor::AbstractFactor, partitions::Vector{LevelPartition},
                     âˆ‚ğ‘™covariate::AbstractCovariate{T}, âˆ‚Â²ğ‘™covariate::AbstractCovariate{T}, slicelength::Integer, singlethread::Bool) where {T<:AbstractFloat}
    
    nodecount = length(nodecansplit)
    levelcounts = [length(p.mask) + 1 for p in partitions]
    fromobs = 1
    toobs = length(nodeids)

    nthreads = singlethread ? 1 : Threads.nthreads()
    threadspace = map((x -> Int64(floor(x))), range(fromobs, toobs, length = nthreads + 1))
    âˆ‚ğ‘™sum = [[(nodecansplit[node] ? [zero(T) for i in 1:(levelcounts[node])] : Vector{T}()) for node in 1:nodecount] for i in 1:nthreads]
    âˆ‚Â²ğ‘™sum = [[(nodecansplit[node] ? [zero(T) for i in 1:(levelcounts[node])] : Vector{T}()) for node in 1:nodecount] for i in 1:nthreads]

    if nthreads > 1
        Threads.@threads for i in 1:nthreads
            sumgradientslice!(âˆ‚ğ‘™sum[i], âˆ‚Â²ğ‘™sum[i], nodeids, nodecansplit, factor,
                            âˆ‚ğ‘™covariate, âˆ‚Â²ğ‘™covariate, (i == 1 ? threadspace[i] : threadspace[i] + 1),
                            threadspace[i + 1], slicelength)
        end
        âˆ‚ğ‘™sum = reduce(+, âˆ‚ğ‘™sum)
        âˆ‚Â²ğ‘™sum = reduce(+, âˆ‚Â²ğ‘™sum)
        [(nodecansplit[node] ? [LossGradient{T}(âˆ‚ğ‘™sum[node][i], âˆ‚Â²ğ‘™sum[node][i]) for i in 1:(levelcounts[node])] : Vector{LossGradient{T}}()) for node in 1:nodecount]
    else
        sumgradientslice!(âˆ‚ğ‘™sum[1], âˆ‚Â²ğ‘™sum[1], nodeids, nodecansplit, factor,
                          âˆ‚ğ‘™covariate, âˆ‚Â²ğ‘™covariate, fromobs, toobs, slicelength)
        [(nodecansplit[node] ? [LossGradient{T}(âˆ‚ğ‘™sum[1][node][i], âˆ‚Â²ğ‘™sum[1][node][i]) for i in 1:(levelcounts[node])] : Vector{LossGradient{T}}()) for node in 1:nodecount]
    end
end

function splitnodeidsslice!(nodeids::Vector{<:Integer}, factors, issplitnode::Vector{Bool}, nodemap::Vector{Int64},
                            leftpartitions::Vector{Vector{Bool}}, factorindex::Vector{Int64},
                            fromobs::Integer, toobs::Integer, slicelength::Integer)
    if length(factors) > 0
        factorslices = zip(Tuple([slice(factor, fromobs, toobs, slicelength) for factor in factors]))
        nodeslices = slice(nodeids, fromobs, toobs, slicelength)
        foreach(zip(nodeslices, factorslices)) do x
            nodeslice, fslices = x
            @inbounds for i in 1:length(nodeslice)
                nodeid = nodeslice[i]
                if issplitnode[nodeid]
                    levelindex = fslices[factorindex[nodeid]][i]
                    nodeslice[i] = (leftpartitions[nodeid][levelindex + 1]) ? nodemap[nodeslice[i]] : nodemap[nodeslice[i]] + 1
                else
                    nodeslice[i] = nodemap[nodeslice[i]]
                end
            end
        end
    end
end

function splitnodeids!(nodeids::Vector{<:Integer}, layer::TreeLayer{T}, slicelength::Integer, singlethread::Bool) where {T<:AbstractFloat}
    nodes = layer.nodes
    nodecount = length(nodes)
    len = length(nodeids)
    fromobs = 1
    toobs = len
    issplitnode = [isa(n, SplitNode) && n.isactive for n in nodes]
    nodemap = Vector{Int64}()
    splitnodecount = 0
    for (i, x) in enumerate(issplitnode)
        push!(nodemap, i + splitnodecount) 
        if x
            splitnodecount += 1
        end
    end
    factors = Vector{AbstractFactor}()
    factorindex = zeros(Int64, nodecount)
    for i in 1:nodecount
         if issplitnode[i]
             factor = nodes[i].factor
             index = findfirst((x -> x == factor), factors)
             if index === nothing
                 push!(factors, factor)
             end
             factorindex[i] = findfirst((x -> x == factor), factors)
         end
    end
    leftpartitions = [isa(n, SplitNode) && n.isactive ? [n.leftnode.partitions[n.factor].inclmissing; n.leftnode.partitions[n.factor].mask] : Vector{Bool}() for n in nodes]

    nthreads = singlethread ? 1 : Threads.nthreads()
    if nthreads > 1
        threadspace = map((x -> Int64(floor(x))), range(fromobs, toobs, length = nthreads + 1))
        Threads.@threads for j in 1:nthreads
             splitnodeidsslice!(nodeids, factors, issplitnode, nodemap, leftpartitions, factorindex,
                                j == 1 ? threadspace[j] : threadspace[j] + 1,
                                threadspace[j + 1], slicelength)
        end
    else
        splitnodeidsslice!(nodeids, factors, issplitnode, nodemap, leftpartitions, factorindex,
                           fromobs, toobs, slicelength)
    end
    nodeids
end

function getsplitnode(factor::AbstractFactor, leafnode::LeafNode{T}, gradient::Vector{LossGradient{T}},
                      Î»::T, minâˆ‚Â²ğ‘™::T, ordstumps::Bool, optsplit::Bool) where {T<:AbstractFloat}

    partition = leafnode.partitions[factor]
    isord = isordinal(factor)
    gradstart = findfirst(partition.mask) + 1
    âˆ‚ğ‘™sum0 = sum((grad -> grad.âˆ‚ğ‘™), gradient[gradstart:end])
    âˆ‚Â²ğ‘™sum0 = sum((grad -> grad.âˆ‚Â²ğ‘™), gradient[gradstart:end]) 
    k = length(gradient) - gradstart + 1
    f = isord ? collect(1:k) : 
        (optsplit ? sortperm([gradient[i].âˆ‚ğ‘™ / gradient[i].âˆ‚Â²ğ‘™ for i in gradstart:length(gradient)]) : collect(1:k))
    missâˆ‚ğ‘™ = gradient[1].âˆ‚ğ‘™ 
    missâˆ‚Â²ğ‘™ = gradient[1].âˆ‚Â²ğ‘™
    currloss = getloss(âˆ‚ğ‘™sum0 + missâˆ‚ğ‘™, âˆ‚Â²ğ‘™sum0 + missâˆ‚Â²ğ‘™, Î»)
    bestloss = typemax(T) 
    levelcount = length(partition.mask)
    leftnode = LeafNode{T}(LossGradient{T}(âˆ‚ğ‘™sum0 + missâˆ‚ğ‘™, âˆ‚Â²ğ‘™sum0 + missâˆ‚Â²ğ‘™), true, copy(leafnode.partitions))
    rightnode = LeafNode{T}(LossGradient{T}(zero(T), zero(T)), true, copy(leafnode.partitions))
    split = SplitNode{T}(factor, leftnode, rightnode, bestloss, false, zero(T))
    leftpartition = deepcopy(leftnode.partitions[factor])
    rightpartition = deepcopy(rightnode.partitions[factor])
    
    leftâˆ‚ğ‘™sum = isord ? gradient[gradstart].âˆ‚ğ‘™ : gradient[gradstart + f[1] - 1].âˆ‚ğ‘™
    leftâˆ‚Â²ğ‘™sum = isord ? gradient[gradstart].âˆ‚Â²ğ‘™ : gradient[gradstart + f[1] - 1].âˆ‚Â²ğ‘™

    firstlevelwithmiss = getloss(leftâˆ‚ğ‘™sum + missâˆ‚ğ‘™, leftâˆ‚Â²ğ‘™sum + missâˆ‚Â²ğ‘™, Î») + getloss(âˆ‚ğ‘™sum0 - leftâˆ‚ğ‘™sum, âˆ‚Â²ğ‘™sum0 - leftâˆ‚Â²ğ‘™sum, Î»)
    firstlevelwitouthmiss = getloss(leftâˆ‚ğ‘™sum, leftâˆ‚Â²ğ‘™sum, Î») + getloss(âˆ‚ğ‘™sum0 - leftâˆ‚ğ‘™sum + missâˆ‚ğ‘™, âˆ‚Â²ğ‘™sum0 - leftâˆ‚Â²ğ‘™sum + missâˆ‚Â²ğ‘™, Î»)

    if firstlevelwithmiss < bestloss && (leftâˆ‚Â²ğ‘™sum + missâˆ‚Â²ğ‘™ >= minâˆ‚Â²ğ‘™) && (âˆ‚Â²ğ‘™sum0 - leftâˆ‚Â²ğ‘™sum >= minâˆ‚Â²ğ‘™)
        if firstlevelwitouthmiss < firstlevelwithmiss && (leftâˆ‚Â²ğ‘™sum >= minâˆ‚Â²ğ‘™) && (âˆ‚Â²ğ‘™sum0 - leftâˆ‚Â²ğ‘™sum + missâˆ‚Â²ğ‘™ >= minâˆ‚Â²ğ‘™)
            split.leftnode.gradient.âˆ‚ğ‘™ = leftâˆ‚ğ‘™sum
            split.leftnode.gradient.âˆ‚Â²ğ‘™ = leftâˆ‚Â²ğ‘™sum
            split.rightnode.gradient.âˆ‚ğ‘™ = âˆ‚ğ‘™sum0 - leftâˆ‚ğ‘™sum + missâˆ‚ğ‘™
            split.rightnode.gradient.âˆ‚Â²ğ‘™ = âˆ‚Â²ğ‘™sum0 - leftâˆ‚Â²ğ‘™sum + missâˆ‚Â²ğ‘™
            for j in (gradstart - 1):levelcount
                leftpartition.mask[j] = j == (gradstart + f[1] - 2)
                rightpartition.mask[j] = j == (gradstart + f[1] - 2) ? false : partition.mask[j]
            end
            leftpartition.inclmissing = false
            rightpartition.inclmissing = partition.inclmissing
            split.loss = firstlevelwitouthmiss
        else
            split.leftnode.gradient.âˆ‚ğ‘™ = leftâˆ‚ğ‘™sum + missâˆ‚ğ‘™
            split.leftnode.gradient.âˆ‚Â²ğ‘™ = leftâˆ‚Â²ğ‘™sum + missâˆ‚Â²ğ‘™
            split.rightnode.gradient.âˆ‚ğ‘™ = âˆ‚ğ‘™sum0 - leftâˆ‚ğ‘™sum
            split.rightnode.gradient.âˆ‚Â²ğ‘™ = âˆ‚Â²ğ‘™sum0 - leftâˆ‚Â²ğ‘™sum
            for j in (gradstart - 1):levelcount
                leftpartition.mask[j] = j == (gradstart + f[1] - 2)
                rightpartition.mask[j] = j == (gradstart + f[1] - 2) ? false : partition.mask[j]
            end
            leftpartition.inclmissing = partition.inclmissing
            rightpartition.inclmissing = false
            split.loss = firstlevelwithmiss
        end
    end

    @inbounds for i in (gradstart + 1):(levelcount + 1)
        fi = isord ? i : (gradstart - 1) + f[(i - gradstart) + 1]
        if !partition.mask[fi - 1]
            continue
        end
        âˆ‚ğ‘™ = gradient[fi].âˆ‚ğ‘™
        âˆ‚Â²ğ‘™ = gradient[fi].âˆ‚Â²ğ‘™

        singlelevelwithmisstotal = getloss(âˆ‚ğ‘™ + missâˆ‚ğ‘™, âˆ‚Â²ğ‘™ + missâˆ‚Â²ğ‘™, Î») + getloss(âˆ‚ğ‘™sum0 - âˆ‚ğ‘™, âˆ‚Â²ğ‘™sum0 - âˆ‚Â²ğ‘™, Î»)
        singlelevelwitouthmisstotal = getloss(âˆ‚ğ‘™, âˆ‚Â²ğ‘™, Î») + getloss(âˆ‚ğ‘™sum0 - âˆ‚ğ‘™ + missâˆ‚ğ‘™, âˆ‚Â²ğ‘™sum0 - âˆ‚Â²ğ‘™ + missâˆ‚Â²ğ‘™, Î»)

        leftâˆ‚ğ‘™sum += âˆ‚ğ‘™
        leftâˆ‚Â²ğ‘™sum += âˆ‚Â²ğ‘™

        leftwithmisstotal = getloss(leftâˆ‚ğ‘™sum + missâˆ‚ğ‘™, leftâˆ‚Â²ğ‘™sum + missâˆ‚Â²ğ‘™, Î») + getloss(âˆ‚ğ‘™sum0 - leftâˆ‚ğ‘™sum, âˆ‚Â²ğ‘™sum0 - leftâˆ‚Â²ğ‘™sum, Î»)
        leftwithoutmisstotal = getloss(leftâˆ‚ğ‘™sum, leftâˆ‚Â²ğ‘™sum, Î») + getloss(âˆ‚ğ‘™sum0 - leftâˆ‚ğ‘™sum + missâˆ‚ğ‘™, âˆ‚Â²ğ‘™sum0 - leftâˆ‚Â²ğ‘™sum + missâˆ‚Â²ğ‘™, Î»)

        if !isord
            if singlelevelwithmisstotal < split.loss && (âˆ‚Â²ğ‘™ + missâˆ‚Â²ğ‘™ >= minâˆ‚Â²ğ‘™) && (âˆ‚Â²ğ‘™sum0 - âˆ‚Â²ğ‘™ >= minâˆ‚Â²ğ‘™)
                if singlelevelwitouthmisstotal < singlelevelwithmisstotal && (âˆ‚Â²ğ‘™ >= minâˆ‚Â²ğ‘™) && (âˆ‚Â²ğ‘™sum0 - âˆ‚Â²ğ‘™ + missâˆ‚Â²ğ‘™ >= minâˆ‚Â²ğ‘™)
                    split.leftnode.gradient.âˆ‚ğ‘™ = âˆ‚ğ‘™
                    split.leftnode.gradient.âˆ‚Â²ğ‘™ =  âˆ‚Â²ğ‘™
                    split.rightnode.gradient.âˆ‚ğ‘™ = âˆ‚ğ‘™sum0 - âˆ‚ğ‘™ + missâˆ‚ğ‘™
                    split.rightnode.gradient.âˆ‚Â²ğ‘™ = âˆ‚Â²ğ‘™sum0 - âˆ‚Â²ğ‘™ + missâˆ‚Â²ğ‘™
                    leftpartition.inclmissing = false
                    rightpartition.inclmissing = partition.inclmissing
                    split.loss = singlelevelwitouthmisstotal
                else
                    split.leftnode.gradient.âˆ‚ğ‘™ = âˆ‚ğ‘™ + missâˆ‚ğ‘™
                    split.leftnode.gradient.âˆ‚Â²ğ‘™ =  âˆ‚Â²ğ‘™ + missâˆ‚Â²ğ‘™
                    split.rightnode.gradient.âˆ‚ğ‘™ = âˆ‚ğ‘™sum0 - âˆ‚ğ‘™
                    split.rightnode.gradient.âˆ‚Â²ğ‘™ = âˆ‚Â²ğ‘™sum0 - âˆ‚Â²ğ‘™
                    leftpartition.inclmissing = partition.inclmissing
                    rightpartition.inclmissing = false
                    split.loss = singlelevelwithmisstotal
                end
                fi = isord ? (i - 1) : (fi - 1)
                for j in (gradstart - 1):levelcount
                    leftpartition.mask[j] = j == fi
                    rightpartition.mask[j] = j == fi ? false : partition.mask[j]
                end
            end
        end
        
        if isord
            if leftwithmisstotal < split.loss && (leftâˆ‚Â²ğ‘™sum + missâˆ‚Â²ğ‘™ >= minâˆ‚Â²ğ‘™) && (âˆ‚Â²ğ‘™sum0 - leftâˆ‚Â²ğ‘™sum >= minâˆ‚Â²ğ‘™)
                if leftwithoutmisstotal < leftwithmisstotal && (leftâˆ‚Â²ğ‘™sum >= minâˆ‚Â²ğ‘™) && (âˆ‚Â²ğ‘™sum0 - leftâˆ‚Â²ğ‘™sum + missâˆ‚Â²ğ‘™ >= minâˆ‚Â²ğ‘™)
                    split.leftnode.gradient.âˆ‚ğ‘™ = leftâˆ‚ğ‘™sum
                    split.leftnode.gradient.âˆ‚Â²ğ‘™ = leftâˆ‚Â²ğ‘™sum
                    split.rightnode.gradient.âˆ‚ğ‘™ = âˆ‚ğ‘™sum0 - leftâˆ‚ğ‘™sum + missâˆ‚ğ‘™
                    split.rightnode.gradient.âˆ‚Â²ğ‘™ = âˆ‚Â²ğ‘™sum0 - leftâˆ‚Â²ğ‘™sum + missâˆ‚Â²ğ‘™
                    leftpartition.inclmissing = false
                    rightpartition.inclmissing = partition.inclmissing
                    split.loss = leftwithoutmisstotal
                else
                    split.leftnode.gradient.âˆ‚ğ‘™ = leftâˆ‚ğ‘™sum + missâˆ‚ğ‘™
                    split.leftnode.gradient.âˆ‚Â²ğ‘™ = leftâˆ‚Â²ğ‘™sum + missâˆ‚Â²ğ‘™
                    split.rightnode.gradient.âˆ‚ğ‘™ = âˆ‚ğ‘™sum0 - leftâˆ‚ğ‘™sum
                    split.rightnode.gradient.âˆ‚Â²ğ‘™ = âˆ‚Â²ğ‘™sum0 - leftâˆ‚Â²ğ‘™sum
                    leftpartition.inclmissing = partition.inclmissing
                    rightpartition.inclmissing = false
                    split.loss = leftwithmisstotal
                end
                for j in (gradstart - 1):levelcount
                    fj = isord ? j : (gradstart - 2) + f[(j - gradstart) + 2]
                    if j <= i - 1
                        leftpartition.mask[fj] = partition.mask[fj]
                        rightpartition.mask[fj] = false
                    else
                        leftpartition.mask[fj] = false
                        rightpartition.mask[fj] = partition.mask[fj]
                    end
                end
            end
        end
    end

    if count(rightpartition.mask) > 0 && split.loss < typemax(T)
        split.gain = currloss - split.loss
        leftnode.partitions[factor] = leftpartition
        leftnode.cansplit = leftnode.gradient.âˆ‚Â²ğ‘™ >= minâˆ‚Â²ğ‘™
        rightnode.partitions[factor] = rightpartition
        rightnode.cansplit = rightnode.gradient.âˆ‚Â²ğ‘™ >= minâˆ‚Â²ğ‘™
        split
    else
        nothing
    end
end

function getnewsplit(gradient::Vector{Vector{LossGradient{T}}}, nodes::Vector{TreeNode{T}}, factor::AbstractFactor,
                     Î»::T, Î³::T, minâˆ‚Â²ğ‘™::T, ordstumps::Bool, optsplit::Bool, singlethread::Bool) where {T<:AbstractFloat}
    newsplit = Vector{Union{SplitNode{T}, Nothing}}(undef, length(gradient))
    if !singlethread && length(gradient) > 2 * Threads.nthreads()
        Threads.@threads for i in 1:length(gradient)
            grad = gradient[i]
            if isa(nodes[i], LeafNode) && nodes[i].cansplit
                partition = nodes[i].partitions[factor]
                if count(partition.mask) > 1
                    newsplit[i] = getsplitnode(factor, nodes[i],  grad, Î», minâˆ‚Â²ğ‘™, ordstumps, optsplit)
                else
                    newsplit[i] = nothing
                end
            else
                newsplit[i] = nothing
            end         
        end
    else
        for i in 1:length(gradient)
            grad = gradient[i]
            if isa(nodes[i], LeafNode) && nodes[i].cansplit
                partition = nodes[i].partitions[factor]
                if count(partition.mask) > 1
                    newsplit[i] = getsplitnode(factor, nodes[i],  grad, Î», minâˆ‚Â²ğ‘™, ordstumps, optsplit)
                else
                    newsplit[i] = nothing
                end
            else
                newsplit[i] = nothing
            end         
        end
    end
    newsplit
end 

function findbestsplit(state::TreeGrowState{T}) where {T<:AbstractFloat}

    nodecansplit = [isa(n, LeafNode) && n.cansplit for n in state.nodes]
    mingain = T(0.5) * state.Î³
    currloss = [getloss(n, state.Î») for n in state.nodes]
    res = foldl(enumerate(state.factors); init = state.nodes) do currsplit, nfactor
        n, factor = nfactor
        partitions = [isa(node, LeafNode) && node.cansplit ? node.partitions[factor] : LevelPartition(Vector{Bool}(), false) for node in state.nodes]

        gradient = sumgradient(state.nodeids, nodecansplit, factor, partitions, state.âˆ‚ğ‘™covariate, state.âˆ‚Â²ğ‘™covariate, state.slicelength, state.singlethread)
        
        newsplit = getnewsplit(gradient, state.nodes, factor, state.Î», state.Î³, state.minâˆ‚Â²ğ‘™, state.ordstumps, state.optsplit, state.singlethread)

        res = Vector{TreeNode{T}}(undef, length(newsplit))
        @inbounds for i in 1:length(newsplit)
            if newsplit[i] !== nothing
               newloss = newsplit[i].loss
               newgain = newsplit[i].gain
               if newgain > mingain && newloss < getloss(currsplit[i], state.Î»)
                   res[i] = newsplit[i]
               else
                   res[i] = currsplit[i] 
               end
            else
               res[i] = currsplit[i] 
            end
        end
        res

        # if state.leafwise
        #     gain = map(getgain, res)
        #     (maxgain, imax) = findmax(gain)
        #     if maxgain > mingain

        #     end

        #     gain = [isnull(newsplit[i]) ? T(NaN32) : currloss[i] - get(newsplit[i]).loss for i in 1:length(newsplit)]
        #     (maxgain, imax) = findmax(gain)
        #     if maxgain > bestgain
        #         bestgain = maxgain
        #         @inbounds for i in 1:length(newsplit)
        #             if i == imax
        #                 res[i] = get(newsplit[i])
        #             else
        #                 res[i] = state.nodes[i] 
        #             end
        #         end
        #     else
        #         @inbounds for i in 1:length(newsplit)
        #                 res[i] = currsplit[i] 
        #         end
        #     end
        # else
        #     mingain = T(0.5) * state.Î³
        #     @inbounds for i in 1:length(newsplit)
        #         if !isnull(newsplit[i])
        #            newloss = get(newsplit[i]).loss
        #            if currloss[i] - newloss > mingain && newloss < getloss(currsplit[i], state.Î»)
        #                res[i] = get(newsplit[i])
        #            else
        #                res[i] = currsplit[i] 
        #            end
        #         else
        #            res[i] = currsplit[i] 
        #         end
        #    end
        #end
    end
    if state.leafwise
        gain = map(getgain, res)
        (maxgain, imax) = findmax(gain)
        if maxgain > mingain
            @inbounds for i in 1:length(res)
                if i == imax && isa(res[i], SplitNode)
                    res[i].isactive = true
                elseif isa(res[i], SplitNode)
                    res[i].isactive = false
                end
            end
        end
    else
        @inbounds for i in 1:length(res)
            if isa(res[i], SplitNode)
                res[i].isactive = true
            end
        end
    end
    res
end

function updatestate(state::TreeGrowState{T}, layer::TreeLayer{T}) where {T<:AbstractFloat}
    splitnodeids!(state.nodeids, layer, state.slicelength, state.singlethread)  
    factors = state.factors
    newnodes = Vector{TreeNode{T}}()
    @inbounds for (i, n) in enumerate(layer.nodes)
        if isa(n, SplitNode) && n.isactive
            push!(newnodes, n.leftnode)
            push!(newnodes, n.rightnode)
            # if isa(state.nodes[i], LeafNode)
            #     leftpartitions = map(state.nodes[i].partitions) do x
            #         f, p = x
            #         if f == n.factor
            #             f => n.leftpartition
            #         else
            #             x
            #         end
            #     end
            #     rightpartitions = map(state.nodes[i].partitions) do x
            #         f, p = x
            #         if f == n.factor
            #             f => n.rightpartition
            #         else
            #             x
            #         end
            #     end
            #     push!(newnodes, LeafNode{T}(n.leftgradient,
            #                                 n.leftgradient.âˆ‚Â²ğ‘™ >= state.minâˆ‚Â²ğ‘™,
            #                                 leftpartitions))
            #     push!(newnodes, LeafNode{T}(n.rightgradient,
            #                                 n.rightgradient.âˆ‚Â²ğ‘™ >= state.minâˆ‚Â²ğ‘™,
            #                                 rightpartitions))
        else
            push!(newnodes, n)
        end
    end
    # activefactors = filter(factors) do f
    #     any(map((n -> count(n.partitions[f].mask) > 1), newnodes))
    # end 
    # state.factors = activefactors
    # for n in newnodes
    #     n.partitions = filter(n.partitions) do f, p
    #         f in activefactors
    #     end
    # end
    state.nodes = newnodes
    state
end

function nextlayer(state::TreeGrowState{T}) where {T<:AbstractFloat}
    layernodes = findbestsplit(state)
    layer = TreeLayer{T}(layernodes)
    updatestate(state, layer)
    layer, state      
end

function predict(treelayer::TreeLayer{T}, nodeids::Vector{<:Integer}, Î»::T) where {T<:AbstractFloat}
    weights = Vector{T}()
    @inbounds for (i, node) in enumerate(treelayer.nodes)
        if isa(node, SplitNode) && node.isactive
            push!(weights, getweight(node.leftnode.gradient, Î»))
            push!(weights, getweight(node.rightnode.gradient, Î»))
        elseif isa(node, SplitNode) 
            push!(weights, getweight(node.leftnode.gradient + node.rightnode.gradient, Î»))
        else
            push!(weights, getweight(node.gradient, Î»))
        end
    end
    (nodeid -> nodeid > 0 ? weights[nodeid] : T(NaN32)).(nodeids)
end

function getlevelmap(fromfactor::AbstractFactor, tofactor::AbstractFactor)
    fromlevels = getlevels(fromfactor)
    tolevels = getlevels(tofactor)
    levelmap = Dict{Int64, Int64}()
    for (i, level) in enumerate(fromlevels)
        j = findfirst((x -> x == level), tolevels)
        if j !== nothing
            levelmap[i] = j
        end
    end
    levelmap
end

function getnewindices(fromfactor::AbstractFactor, tofactor::AbstractFactor)
    fromlevels = getlevels(fromfactor)
    tolevels = getlevels(tofactor)
    newind = Set{Int64}()
    for (i, level) in enumerate(tolevels)
        j = findfirst((x -> x == level), fromlevels)
        if j !== nothing
            push!(newind, i)
        end
    end
    newind
end

function Base.map(node::SplitNode{T}, dataframe::AbstractDataFrame, 
                  factormap::Dict{AbstractFactor, Tuple{AbstractFactor, Dict{Int64, Int64}, Set{Int64}, Int64}}) where {T<:AbstractFloat}
    
    factor, levelmap, newind, levelcount = factormap[node.factor]
    leftmask = Vector{Bool}(undef, levelcount)
    rightmask = Vector{Bool}(undef, levelcount)
    for (i, j) in levelmap
        leftmask[j] = node.leftnode.partitions[node.factor].mask[i] 
        rightmask[j] = node.rightnode.partitions[node.factor].mask[i] 
    end
    for i in newind
        leftmask[i] = false
        rightmask[i] = true
    end
    node.leftnode.partitions[factor] = LevelPartition(leftmask, node.leftnode.partitions[node.factor].inclmissing)
    node.rightnode.partitions[factor] = LevelPartition(rightmask, node.rightnode.partitions[node.factor].inclmissing)
    SplitNode{T}(factor, node.leftnode, node.rightnode, node.loss, node.isactive, node.gain)
end

function Base.map(node::LeafNode{T}, dataframe::AbstractDataFrame,
                  factormap::Dict{AbstractFactor, Tuple{AbstractFactor, Dict{Int64, Int64}, Set{Int64}, Int64}}) where {T<:AbstractFloat}
    node
end

function predict(tree::XGTree{T}, dataframe::AbstractDataFrame) where {T<:AbstractFloat}
    len = length(dataframe)
    maxnodecount = tree.leafwise ? tree.maxleaves : 2 ^ tree.maxdepth
    nodeids = maxnodecount <= typemax(UInt8) ? ones(UInt8, len) : (maxnodecount <= typemax(UInt16) ? ones(UInt16, len) : ones(UInt32, len))
    for layer in tree.layers
        splitnodeids!(nodeids, layer, tree.slicelength, tree.singlethread)
    end
    predict(tree.layers[end], nodeids, tree.Î»)
end

function growtree(factors::Vector{<:AbstractFactor}, âˆ‚ğ‘™covariate::AbstractCovariate{T},
                  âˆ‚Â²ğ‘™covariate::AbstractCovariate{T}, maxdepth::Integer, Î»::T, Î³::T, leafwise::Bool, maxleaves::Integer,
                  minâˆ‚Â²ğ‘™::T, ordstumps::Bool, optsplit::Bool, pruning::Bool, slicelength::Integer, singlethread::Bool) where {T<:AbstractFloat}

    len = length(âˆ‚ğ‘™covariate)
    maxnodecount = leafwise ? maxleaves : 2 ^ maxdepth
    maxsteps = leafwise ? (maxleaves - 1) : maxdepth
    nodeids = maxnodecount <= typemax(UInt8) ? ones(UInt8, len) : (maxnodecount <= typemax(UInt16) ? ones(UInt16, len) : ones(UInt32, len))
    intercept = ConstFactor(len)
    @time grad0 = sumgradient(nodeids, [true], intercept, [LevelPartition([true], false)], âˆ‚ğ‘™covariate, âˆ‚Â²ğ‘™covariate, slicelength, singlethread)[1][1]
    nodes0 = Vector{TreeNode{T}}()
    push!(nodes0, LeafNode{T}(grad0, true, Dict([f => LevelPartition(ones(Bool, length(getlevels(f))), true) for f in factors])))
    state0 = TreeGrowState{T}(nodeids, nodes0, factors, âˆ‚ğ‘™covariate, âˆ‚Â²ğ‘™covariate, Î», Î³, minâˆ‚Â²ğ‘™, ordstumps, optsplit, pruning, leafwise, slicelength, singlethread)
    @time layers = collect(Iterators.take(Seq(TreeLayer{T}, state0, nextlayer), maxsteps))
    xgtree = XGTree{T}(layers, Î», Î³, minâˆ‚Â²ğ‘™, maxdepth, leafwise, maxleaves, slicelength, singlethread)
    if pruning
        tree = convert(Tree{TreeNode{T}}, xgtree)
        pruned = prune(tree, Î», Î³)
        prunedlayers = map((nodes -> TreeLayer{T}(nodes)) , convert(Vector{Vector{TreeNode{T}}}, convert(List{List{TreeNode{T}}}, rebalance(pruned, maxdepth))))
        xgtree = XGTree{T}(prunedlayers, Î», Î³, minâˆ‚Â²ğ‘™, maxdepth, leafwise, maxleaves, slicelength, singlethread)
        pred = predict(xgtree.layers[end], nodeids, Î»)
        xgtree, pred
    else
        pred = predict(xgtree.layers[end], nodeids, Î»)
        xgtree, pred
    end
end

function Base.convert(::Type{Tree{TreeNode{T}}}, xgtree::XGTree{T}) where {T<:AbstractFloat} 
    layers = xgtree.layers
    maxdepth = xgtree.maxdepth
    gettree  = (depth::Integer, nodeid::Integer) -> 
        begin
            node = layers[depth].nodes[nodeid]
            if depth == maxdepth
                ConsTree{TreeNode{T}}(node)
            else
                lefttree = gettree(depth + 1, 2 * nodeid - 1)
                righttree = gettree(depth + 1, 2 * nodeid)
                ConsTree{TreeNode{T}}(node, lefttree, righttree)
            end
        end
    gettree(1, 1)
end

function Base.convert(::Type{List{List{TreeNode{T}}}}, tree::EmptyTree{TreeNode{T}}) where {T<:AbstractFloat}
    EmptyList{EmptyList{TreeNode{T}}}()
end

function Base.convert(::Type{List{List{TreeNode{T}}}}, tree::ConsTree{TreeNode{T}}) where {T<:AbstractFloat}
    node = tree.value
    left = convert(List{List{TreeNode{T}}}, tree.lefttree)
    right = convert(List{List{TreeNode{T}}}, tree.righttree)
    ConsList{List{TreeNode{T}}}(ConsList{TreeNode{T}}(node), map((x -> x[1] + x[2]), zip(left, right), List{TreeNode{T}}))
end

function prune(node::SplitNode{T}, Î»::T, Î³::T) where {T<:AbstractFloat}
    sumgrad = LossGradient(node.leftgradient.âˆ‚ğ‘™ + node.rightgradient.âˆ‚ğ‘™, node.leftgradient.âˆ‚Â²ğ‘™ + node.rightgradient.âˆ‚Â²ğ‘™)
    totloss = getloss(sumgrad.âˆ‚ğ‘™, sumgrad.âˆ‚Â²ğ‘™, Î»)
    if totloss - node.loss > Î³ * T(0.5)
        node::TreeNode{T}
    else
        LeafNode(sumgrad, false, Dict{AbstractFactor, LevelPartition}())::TreeNode{T}
    end
end

function prune(node::LeafNode{T}, Î»::T, Î³::T) where {T<:AbstractFloat}
    node
end

function prune(tree::ConsTree{<:TreeNode{T}}, Î»::T, Î³::T) where {T<:AbstractFloat}
    node = tree.value
    if isempty(tree.lefttree) && isempty(tree.righttree)
        ConsTree{TreeNode{T}}(prune(node, Î», Î³))
    else
        prunednode = prune(node, Î», Î³)
        if isa(prunednode, LeafNode{T})
            ConsTree{TreeNode{T}}(prunednode)
        else
            left = prune(tree.lefttree, Î», Î³)
            right = prune(tree.righttree, Î», Î³)
            ConsTree{TreeNode{T}}(prunednode, left, right)
        end
    end
end

function rebalance(prunedtree::ConsTree{TreeNode{T}}, maxdepth::Integer) where {T<:AbstractFloat}
    if maxdepth == 1
        prunedtree
    else
        if isempty(prunedtree.lefttree) && isempty(prunedtree.righttree) 
            left = rebalance(ConsTree{TreeNode{T}}(prunedtree.value), maxdepth - 1)
            right = rebalance(ConsTree{TreeNode{T}}(prunedtree.value), maxdepth - 1)
            ConsTree{TreeNode{T}}(prunedtree.value, left, right)
        else
            left = rebalance(prunedtree.lefttree, maxdepth - 1)
            right = rebalance(prunedtree.righttree, maxdepth - 1)
            ConsTree{TreeNode{T}}(prunedtree.value, left, right)
        end
    end
end

