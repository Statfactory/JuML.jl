
function getweight(gradient::LossGradient{T}, Î»::T) where {T<:AbstractFloat} 
    -gradient.âˆ‚ğ‘™ / (gradient.âˆ‚Â²ğ‘™ + Î»)
end

function getloss(âˆ‚ğ‘™::T, âˆ‚Â²ğ‘™::T, Î»::T, Î³::T) where {T<:AbstractFloat} 
    -0.5 * âˆ‚ğ‘™ * âˆ‚ğ‘™ / (âˆ‚Â²ğ‘™ + Î») + Î³
end

function getloss(node::LeafNode{T}, Î»::T, Î³::T) where {T<:AbstractFloat} 
    âˆ‚ğ‘™ = node.gradient.âˆ‚ğ‘™
    âˆ‚Â²ğ‘™ = node.gradient.âˆ‚Â²ğ‘™
    getloss(âˆ‚ğ‘™, âˆ‚Â²ğ‘™, Î», Î³)
end

function getloss(node::SplitNode{T}, Î»::T, Î³::T) where {T<:AbstractFloat} 
    node.loss
end

function sumgradientslice!(âˆ‚ğ‘™sum0, âˆ‚Â²ğ‘™sum0, nodeids::Vector{<:Integer}, nodecansplit::Vector{Bool}, factor::AbstractFactor, inclmiss::Vector{Bool},
                           âˆ‚ğ‘™covariate::AbstractCovariate, âˆ‚Â²ğ‘™covariate::AbstractCovariate, fromobs::Integer, toobs::Integer, slicelength::Integer)

    nodeslices = slice(nodeids, fromobs, toobs, slicelength)
    factorslices = slice(factor, fromobs, toobs, slicelength)
    âˆ‚ğ‘™slices = slice(âˆ‚ğ‘™covariate, fromobs, toobs, slicelength)
    âˆ‚Â²ğ‘™slices = slice(âˆ‚Â²ğ‘™covariate, fromobs, toobs, slicelength)
    zipslices = zip4(nodeslices, factorslices, âˆ‚ğ‘™slices, âˆ‚Â²ğ‘™slices)
    fold((âˆ‚ğ‘™sum0, âˆ‚Â²ğ‘™sum0), zipslices) do gradsum, zipslice
        nodeslice, factorslice, âˆ‚ğ‘™slice, âˆ‚Â²ğ‘™slice = zipslice
        âˆ‚ğ‘™sum, âˆ‚Â²ğ‘™sum = gradsum
        @inbounds for i in 1:length(nodeslice)
            nodeid = nodeslice[i]
            if nodecansplit[nodeid]
                levelindex = factorslice[i]
                if levelindex == 0
                    âˆ‚ğ‘™sum[nodeid][1] += âˆ‚ğ‘™slice[i]
                    âˆ‚Â²ğ‘™sum[nodeid][1] += âˆ‚Â²ğ‘™slice[i]
                elseif inclmiss[nodeid]
                    âˆ‚ğ‘™sum[nodeid][levelindex + 1] += âˆ‚ğ‘™slice[i]
                    âˆ‚Â²ğ‘™sum[nodeid][levelindex + 1] += âˆ‚Â²ğ‘™slice[i]
                else
                    âˆ‚ğ‘™sum[nodeid][levelindex] += âˆ‚ğ‘™slice[i]
                    âˆ‚Â²ğ‘™sum[nodeid][levelindex] += âˆ‚Â²ğ‘™slice[i]
                end
            end
        end
        (âˆ‚ğ‘™sum, âˆ‚Â²ğ‘™sum)
    end
end

function sumgradient(nodeids::Vector{<:Integer}, nodecansplit::Vector{Bool}, factor::AbstractFactor, partitions::Vector{LevelPartition},
                     âˆ‚ğ‘™covariate::AbstractCovariate{T}, âˆ‚Â²ğ‘™covariate::AbstractCovariate{T}, slicelength::Integer, singlethread::Bool) where {T<:AbstractFloat}
    
    nodecount = length(nodecansplit)
    levelcounts = [p.inclmissing ? length(p.mask) + 1 : length(p.mask) for p in partitions]
    inclmiss = [p.inclmissing for p in partitions]
    fromobs = 1
    toobs = length(nodeids)

    nthreads = singlethread ? 1 : Threads.nthreads()
    threadspace = map((x -> Int64(floor(x))), LinSpace(fromobs, toobs, nthreads + 1))
    âˆ‚ğ‘™sum = [[(nodecansplit[node] ? [zero(T) for i in 1:(levelcounts[node])] : Vector{T}()) for node in 1:nodecount] for i in 1:nthreads]
    âˆ‚Â²ğ‘™sum = [[(nodecansplit[node] ? [zero(T) for i in 1:(levelcounts[node])] : Vector{T}()) for node in 1:nodecount] for i in 1:nthreads]

    if nthreads > 1
        Threads.@threads for i in 1:nthreads
            sumgradientslice!(âˆ‚ğ‘™sum[i], âˆ‚Â²ğ‘™sum[i], nodeids, nodecansplit, factor, inclmiss,
                            âˆ‚ğ‘™covariate, âˆ‚Â²ğ‘™covariate, threadspace[i],
                            i == nthreads ? threadspace[i + 1] : threadspace[i + 1] - 1, slicelength)
        end
        âˆ‚ğ‘™sum = reduce(+, âˆ‚ğ‘™sum)
        âˆ‚Â²ğ‘™sum = reduce(+, âˆ‚Â²ğ‘™sum)
        [(nodecansplit[node] ? [LossGradient{T}(âˆ‚ğ‘™sum[node][i], âˆ‚Â²ğ‘™sum[node][i]) for i in 1:(levelcounts[node])] : Vector{LossGradient{T}}()) for node in 1:nodecount]
    else
        sumgradientslice!(âˆ‚ğ‘™sum[1], âˆ‚Â²ğ‘™sum[1], nodeids, nodecansplit, factor, inclmiss,
                          âˆ‚ğ‘™covariate, âˆ‚Â²ğ‘™covariate, fromobs, toobs, slicelength)
        [(nodecansplit[node] ? [LossGradient{T}(âˆ‚ğ‘™sum[1][node][i], âˆ‚Â²ğ‘™sum[1][node][i]) for i in 1:(levelcounts[node])] : Vector{LossGradient{T}}()) for node in 1:nodecount]
    end
end

function splitnodeidsslice!(nodeids::Vector{<:Integer}, factors, issplitnode::Vector{Bool},
                            leftpartitions::Vector{LevelPartition}, levelcounts::Vector{Int64}, factorindex::Vector{Int64},
                            fromobs::Integer, toobs::Integer, slicelength::Integer)
    if length(factors) == 0
        for i in fromobs:toobs
            nodeids[i] = 2 * nodeids[i] - 1
        end
    else
        factorslices = zipn([slice(factor, fromobs, toobs, slicelength) for factor in factors])
        nodeslices = slice(nodeids, fromobs, toobs, slicelength)
        foreach(zip2(nodeslices, factorslices)) do x
            nodeslice, fslices = x
            @inbounds for i in 1:length(nodeslice)
                nodeid = nodeslice[i]
                if nodeid > 0
                    if issplitnode[nodeid]
                        levelindex = fslices[factorindex[nodeid]][i]
                        if levelindex > levelcounts[nodeid]
                            nodeslice[i] = 0
                        else
                            leftpartition = leftpartitions[nodeid]
                            misswithleft = leftpartition.inclmissing
                            nodeslice[i] = (levelindex == 0 && misswithleft) || leftpartition.mask[levelindex] ? (2 * nodeslice[i] - 1) : (2 * nodeslice[i]) 
                        end
                    else
                        nodeslice[i] = 2 * nodeslice[i] - 1
                    end
                end
            end
        end
    end
end

function splitnodeids!(nodeids::Vector{<:Integer}, layer::TreeLayer{T}, slicelength::Integer, singlethread::Bool) where {T<:AbstractFloat}
    nodes = layer.nodes
    nodecount = length(nodes)
    len = length(nodeids)
    fromobs = 1
    toobs = len
    issplitnode = [isa(n, SplitNode) for n in nodes]
    factors = Vector{AbstractFactor}()
    factorindex = Vector{Int64}(nodecount)
    for i in 1:nodecount
         if issplitnode[i]
             factor = nodes[i].factor
             index = findfirst(factors, factor)
             if index == 0
                 push!(factors, factor)
             end
             factorindex[i] = findfirst(factors, factor)
         end
    end
    factors = widenfactors(factors)
    leftpartitions = [isa(n, SplitNode) ? n.leftpartition : LevelPartition(Vector{Bool}(), false)  for n in nodes]
    levelcounts = [length(p.mask) for p in leftpartitions]

    nthreads = singlethread ? 1 : Threads.nthreads()
    if nthreads > 1
        threadspace = map((x -> Int64(floor(x))), LinSpace(fromobs, toobs, nthreads + 1))
        Threads.@threads for j in 1:nthreads
             splitnodeidsslice!(nodeids, factors, issplitnode, leftpartitions, levelcounts, factorindex,
                                threadspace[j],
                                j == nthreads ? threadspace[j + 1] : (threadspace[j + 1] - 1), slicelength)
        end
    else
        splitnodeidsslice!(nodeids, factors, issplitnode, leftpartitions, levelcounts, factorindex,
                           fromobs, toobs, slicelength)
    end
    nodeids
end

function getsplitnode(factor::AbstractFactor, partition::LevelPartition, gradient::Vector{LossGradient{T}},
                      Î»::T, Î³::T, minâˆ‚Â²ğ‘™::T) where {T<:AbstractFloat}

    inclmiss = partition.inclmissing
    isord = isordinal(factor)
    gradstart = inclmiss ? 2 : 1
    âˆ‚ğ‘™sum0 = sum((grad -> grad.âˆ‚ğ‘™), gradient[gradstart:end])
    âˆ‚Â²ğ‘™sum0 = sum((grad -> grad.âˆ‚Â²ğ‘™), gradient[gradstart:end]) 
    missâˆ‚ğ‘™ = inclmiss ? gradient[1].âˆ‚ğ‘™ : zero(T)
    missâˆ‚Â²ğ‘™ = inclmiss ? gradient[1].âˆ‚Â²ğ‘™ :zero(T)
    bestloss = getloss(âˆ‚ğ‘™sum0 + missâˆ‚ğ‘™, âˆ‚Â²ğ‘™sum0 + missâˆ‚Â²ğ‘™, Î», Î³)
    levelcount = length(partition.mask)
    split = SplitNode{T}(factor, partition, LevelPartition(zeros(Bool, levelcount), false),
                         LossGradient{T}(âˆ‚ğ‘™sum0 + missâˆ‚ğ‘™, âˆ‚Â²ğ‘™sum0 + missâˆ‚Â²ğ‘™), LossGradient{T}(zero(T), zero(T)),
                         bestloss)
    
    leftâˆ‚ğ‘™sum = gradient[gradstart].âˆ‚ğ‘™
    leftâˆ‚Â²ğ‘™sum = gradient[gradstart].âˆ‚Â²ğ‘™

    firstlevelwithmiss = getloss(leftâˆ‚ğ‘™sum + missâˆ‚ğ‘™, leftâˆ‚Â²ğ‘™sum + missâˆ‚Â²ğ‘™, Î», Î³) + getloss(âˆ‚ğ‘™sum0 - leftâˆ‚ğ‘™sum, âˆ‚Â²ğ‘™sum0 - leftâˆ‚Â²ğ‘™sum, Î», Î³)
    firstlevelwitouthmiss = getloss(leftâˆ‚ğ‘™sum, leftâˆ‚Â²ğ‘™sum, Î», Î³) + getloss(âˆ‚ğ‘™sum0 - leftâˆ‚ğ‘™sum + missâˆ‚ğ‘™, âˆ‚Â²ğ‘™sum0 - leftâˆ‚Â²ğ‘™sum + missâˆ‚Â²ğ‘™, Î», Î³)

    if firstlevelwithmiss < bestloss && (leftâˆ‚Â²ğ‘™sum + missâˆ‚Â²ğ‘™ >= minâˆ‚Â²ğ‘™) && (âˆ‚Â²ğ‘™sum0 - leftâˆ‚Â²ğ‘™sum >= minâˆ‚Â²ğ‘™)
        if firstlevelwitouthmiss < firstlevelwithmiss && (leftâˆ‚Â²ğ‘™sum >= minâˆ‚Â²ğ‘™) && (âˆ‚Â²ğ‘™sum0 - leftâˆ‚Â²ğ‘™sum + missâˆ‚Â²ğ‘™ >= minâˆ‚Â²ğ‘™)
            split.leftgradient = LossGradient{T}(leftâˆ‚ğ‘™sum, leftâˆ‚Â²ğ‘™sum)
            split.rightgradient = LossGradient{T}(âˆ‚ğ‘™sum0 - leftâˆ‚ğ‘™sum + missâˆ‚ğ‘™, âˆ‚Â²ğ‘™sum0 - leftâˆ‚Â²ğ‘™sum + missâˆ‚Â²ğ‘™)
            split.leftpartition = LevelPartition([j == 1 for j in 1:levelcount], false)
            split.rightpartition = LevelPartition([j == 1 ? false : partition.mask[j] for j in 1:levelcount], partition.inclmissing)
            split.loss = firstlevelwitouthmiss
        else
            split.leftgradient = LossGradient{T}(leftâˆ‚ğ‘™sum + missâˆ‚ğ‘™, leftâˆ‚Â²ğ‘™sum + missâˆ‚Â²ğ‘™)
            split.rightgradient = LossGradient{T}(âˆ‚ğ‘™sum0 - leftâˆ‚ğ‘™sum, âˆ‚Â²ğ‘™sum0 - leftâˆ‚Â²ğ‘™sum)
            split.leftpartition = LevelPartition([j == 1 for j in 1:levelcount], partition.inclmissing)
            split.rightpartition = LevelPartition([j == 1 ? false : partition.mask[j] for j in 1:levelcount], partition.inclmissing)
            split.loss = firstlevelwithmiss
        end
    end

    @inbounds for i in 2:(levelcount - 1)
        if !partition.mask[i]
            continue
        end
        âˆ‚ğ‘™ = gradient[i + gradstart - 1].âˆ‚ğ‘™
        âˆ‚Â²ğ‘™ = gradient[i + gradstart - 1].âˆ‚Â²ğ‘™

        singlelevelwithmisstotal = getloss(âˆ‚ğ‘™ + missâˆ‚ğ‘™, âˆ‚Â²ğ‘™ + missâˆ‚Â²ğ‘™, Î», Î³) + getloss(âˆ‚ğ‘™sum0 - âˆ‚ğ‘™, âˆ‚Â²ğ‘™sum0 - âˆ‚Â²ğ‘™, Î», Î³)
        singlelevelwitouthmisstotal = getloss(âˆ‚ğ‘™, âˆ‚Â²ğ‘™, Î», Î³) + getloss(âˆ‚ğ‘™sum0 - âˆ‚ğ‘™ + missâˆ‚ğ‘™, âˆ‚Â²ğ‘™sum0 - âˆ‚Â²ğ‘™ + missâˆ‚Â²ğ‘™, Î», Î³)

        leftâˆ‚ğ‘™sum += âˆ‚ğ‘™
        leftâˆ‚Â²ğ‘™sum += âˆ‚Â²ğ‘™

        leftwithmisstotal = getloss(leftâˆ‚ğ‘™sum + missâˆ‚ğ‘™, leftâˆ‚Â²ğ‘™sum + missâˆ‚Â²ğ‘™, Î», Î³) + getloss(âˆ‚ğ‘™sum0 - leftâˆ‚ğ‘™sum, âˆ‚Â²ğ‘™sum0 - leftâˆ‚Â²ğ‘™sum, Î», Î³)
        leftwithoutmisstotal = getloss(leftâˆ‚ğ‘™sum, leftâˆ‚Â²ğ‘™sum, Î», Î³) + getloss(âˆ‚ğ‘™sum0 - leftâˆ‚ğ‘™sum + missâˆ‚ğ‘™, âˆ‚Â²ğ‘™sum0 - leftâˆ‚Â²ğ‘™sum + missâˆ‚Â²ğ‘™, Î», Î³)

        if isord
            if leftwithmisstotal < split.loss && (leftâˆ‚Â²ğ‘™sum + missâˆ‚Â²ğ‘™ >= minâˆ‚Â²ğ‘™) && (âˆ‚Â²ğ‘™sum0 - leftâˆ‚Â²ğ‘™sum >= minâˆ‚Â²ğ‘™)
                if leftwithoutmisstotal < leftwithmisstotal && (leftâˆ‚Â²ğ‘™sum >= minâˆ‚Â²ğ‘™) && (âˆ‚Â²ğ‘™sum0 - leftâˆ‚Â²ğ‘™sum + missâˆ‚Â²ğ‘™ >= minâˆ‚Â²ğ‘™)
                    split.leftgradient = LossGradient{T}(leftâˆ‚ğ‘™sum, leftâˆ‚Â²ğ‘™sum)
                    split.rightgradient = LossGradient{T}(âˆ‚ğ‘™sum0 - leftâˆ‚ğ‘™sum + missâˆ‚ğ‘™, âˆ‚Â²ğ‘™sum0 - leftâˆ‚Â²ğ‘™sum + missâˆ‚Â²ğ‘™)
                    split.leftpartition = LevelPartition([(j <= i) ? partition.mask[j] : false for j in 1:levelcount], false)
                    split.rightpartition = LevelPartition([j <= i ? false : partition.mask[j] for j in 1:levelcount], partition.inclmissing)
                    split.loss = leftwithoutmisstotal
                else
                    split.leftgradient = LossGradient{T}(leftâˆ‚ğ‘™sum + missâˆ‚ğ‘™, leftâˆ‚Â²ğ‘™sum + missâˆ‚Â²ğ‘™)
                    split.rightgradient = LossGradient{T}(âˆ‚ğ‘™sum0 - leftâˆ‚ğ‘™sum, âˆ‚Â²ğ‘™sum0 - leftâˆ‚Â²ğ‘™sum)
                    split.leftpartition = LevelPartition([(j <= i) ? partition.mask[j] : false for j in 1:levelcount], partition.inclmissing)
                    split.rightpartition = LevelPartition([j <= i ? false : partition.mask[j] for j in 1:levelcount], partition.inclmissing)
                    split.loss = leftwithmisstotal
                end
            end
        else
            if singlelevelwithmisstotal < split.loss && (âˆ‚Â²ğ‘™ + missâˆ‚Â²ğ‘™ >= minâˆ‚Â²ğ‘™) && (âˆ‚Â²ğ‘™sum0 - âˆ‚Â²ğ‘™ >= minâˆ‚Â²ğ‘™)
                if singlelevelwitouthmisstotal < singlelevelwithmisstotal && (âˆ‚Â²ğ‘™ >= minâˆ‚Â²ğ‘™) && (âˆ‚Â²ğ‘™sum0 - âˆ‚Â²ğ‘™ + missâˆ‚Â²ğ‘™ >= minâˆ‚Â²ğ‘™)
                    split.leftgradient = LossGradient{T}(âˆ‚ğ‘™, âˆ‚Â²ğ‘™)
                    split.rightgradient = LossGradient{T}(âˆ‚ğ‘™sum0 - âˆ‚ğ‘™ + missâˆ‚ğ‘™, âˆ‚Â²ğ‘™sum0 - âˆ‚Â²ğ‘™ + missâˆ‚Â²ğ‘™)
                    split.leftpartition = LevelPartition([j == i for j in 1:levelcount], false)
                    split.rightpartition = LevelPartition([j == i ? false : partition.mask[j] for j in 1:levelcount], partition.inclmissing)
                    split.loss = singlelevelwitouthmisstotal
                else
                    split.leftgradient = LossGradient{T}(âˆ‚ğ‘™ + missâˆ‚ğ‘™, âˆ‚Â²ğ‘™ + missâˆ‚Â²ğ‘™)
                    split.rightgradient = LossGradient{T}(âˆ‚ğ‘™sum0 - âˆ‚ğ‘™, âˆ‚Â²ğ‘™sum0 - âˆ‚Â²ğ‘™)
                    split.leftpartition = LevelPartition([j == i for j in 1:levelcount], partition.inclmissing)
                    split.rightpartition = LevelPartition([j == i ? false : partition.mask[j] for j in 1:levelcount], partition.inclmissing)
                    split.loss = singlelevelwithmisstotal
                end
            end
        end
    end
    if count(split.rightpartition.mask) > 0
        Nullable{SplitNode{T}}(split)
    else
        Nullable{SplitNode{T}}()
    end
end

function getnewsplit(gradient::Vector{Vector{LossGradient{T}}}, nodes::Vector{TreeNode{T}}, factor::AbstractFactor,
                     Î»::T, Î³::T, minâˆ‚Â²ğ‘™::T, singlethread::Bool) where {T<:AbstractFloat}
    newsplit = Vector{Nullable{SplitNode{T}}}(length(gradient))
    if !singlethread
        Threads.@threads for i in 1:length(gradient)
            grad = gradient[i]
            if nodes[i].cansplit
                partition = nodes[i].partitions[factor]
                if count(partition.mask) > 1
                    newsplit[i] = getsplitnode(factor, nodes[i].partitions[factor],  grad, Î», Î³, minâˆ‚Â²ğ‘™)
                else
                    newsplit[i] = Nullable{SplitNode{T}}()
                end
            else
                newsplit[i] = Nullable{SplitNode{T}}()
            end         
        end
    else
        for i in 1:length(gradient)
            grad = gradient[i]
            if nodes[i].cansplit
                partition = nodes[i].partitions[factor]
                if count(partition.mask) > 1
                    newsplit[i] = getsplitnode(factor, nodes[i].partitions[factor],  grad, Î», Î³, minâˆ‚Â²ğ‘™)
                else
                    newsplit[i] = Nullable{SplitNode{T}}()
                end
            else
                newsplit[i] = Nullable{SplitNode{T}}()
            end         
        end
    end
    newsplit
end 

function findbestsplit(state::TreeGrowState{T}) where {T<:AbstractFloat}

    nodecansplit = [n.cansplit for n in state.nodes]
    foldl(state.nodes, enumerate(state.factors)) do currsplit, nfactor
        n, factor = nfactor
        partitions = [node.partitions[factor] for node in state.nodes]

        gradient = sumgradient(state.nodeids, nodecansplit, factor, partitions, state.âˆ‚ğ‘™covariate, state.âˆ‚Â²ğ‘™covariate, state.slicelength, state.singlethread)
        
        newsplit = getnewsplit(gradient, state.nodes, factor, state.Î», state.Î³, state.minâˆ‚Â²ğ‘™, state.singlethread)

        res = Vector{TreeNode{T}}(length(newsplit))
        for i in 1:length(newsplit)
             if !isnull(newsplit[i]) && get(newsplit[i]).loss < getloss(currsplit[i], state.Î», state.Î³) 
                res[i] = get(newsplit[i])  
             else
                res[i] = currsplit[i] 
             end
        end
        res
    end
end

function updatestate(state::TreeGrowState{T}, layer::TreeLayer{T}) where {T<:AbstractFloat}
    splitnodeids!(state.nodeids, layer, state.slicelength, state.singlethread)  
    factors = state.factors
    newnodes = Vector{LeafNode{T}}(2 * length(state.nodes))
    @inbounds for (i, n) in enumerate(layer.nodes)
        if isa(n, SplitNode)
            leftpartitions = map(state.nodes[i].partitions) do x
                f, p = x
                if f == n.factor
                    f => n.leftpartition
                else
                    x
                end
            end
            rightpartitions = map(state.nodes[i].partitions) do x
                f, p = x
                if f == n.factor
                    f => n.rightpartition
                else
                    x
                end
            end
            newnodes[2 * i - 1] = LeafNode{T}(n.leftgradient,
                                              n.leftgradient.âˆ‚Â²ğ‘™ >= state.minâˆ‚Â²ğ‘™,
                                              leftpartitions)
            newnodes[2 * i] = LeafNode{T}(n.rightgradient,
                                          n.rightgradient.âˆ‚Â²ğ‘™ >= state.minâˆ‚Â²ğ‘™,
                                          rightpartitions)
        else
            newnodes[2 * i - 1] = LeafNode{T}(n.gradient, false, n.partitions)
            newnodes[2 * i] = LeafNode{T}(n.gradient, false, n.partitions)
        end
    end
    activefactors = filter(factors) do f
        any(map((n -> count(n.partitions[f].mask) > 1), newnodes))
    end 
    state.factors = activefactors
    for n in newnodes
        n.partitions = filter(n.partitions) do f, p
            f in activefactors
        end
    end
    state.nodes = newnodes
    state
end

function nextlayer(state::TreeGrowState{T}) where {T<:AbstractFloat}
    layernodes = findbestsplit(state)
    layer = TreeLayer{T}(layernodes)
    updatestate(state, layer)
    Nullable{TreeLayer{T}}(layer), state      
end

function predict(treelayer::TreeLayer{T}, nodeids::Vector{<:Integer}, Î»::T) where {T<:AbstractFloat}
    weights = Vector{T}(2 * length(treelayer.nodes))
    @inbounds for (i, node) in enumerate(treelayer.nodes)
        if isa(node, SplitNode)
            weights[2 * i - 1] = getweight(node.leftgradient, Î»)
            weights[2 * i] = getweight(node.rightgradient, Î»)
        else
            weights[2 * i - 1] = getweight(node.gradient, Î»)
            weights[2 * i] = getweight(node.gradient, Î»)
        end
    end
    (nodeid -> nodeid > 0 ? weights[nodeid] : T(NaN32)).(nodeids)
end

function predict(tree::Tree{T}, dataframe::AbstractDataFrame) where {T<:AbstractFloat}
    len = length(dataframe)
    maxnodecount = 2 ^ tree.maxdepth
    nodeids = maxnodecount <= typemax(UInt8) ? ones(UInt8, len) : (maxnodecount <= typemax(UInt16) ? ones(UInt16, len) : ones(UInt32, len))
    nodes = Vector{TreeNode{T}}()
    factormap = Dict{AbstractFactor, AbstractFactor}()
    for layer in tree.layers
        for node in layer.nodes
            if isa(node, SplitNode) && !(node.factor in keys(factormap))
                factormap[node.factor] = map(node.factor, dataframe) |> cache
            end
        end
    end

    for layer in tree.layers
        nodes = [isa(n, SplitNode) ? SplitNode{T}(factormap[n.factor], n.leftpartition, n.rightpartition, n.leftgradient, n.rightgradient, n.loss) : n for n in layer.nodes]
        splitnodeids!(nodeids, TreeLayer{T}(nodes), tree.slicelength, tree.singlethread)
    end
    predict(TreeLayer{T}(nodes), nodeids, tree.Î»)
end

function growtree(factors::Vector{<:AbstractFactor}, âˆ‚ğ‘™covariate::AbstractCovariate{T},
                  âˆ‚Â²ğ‘™covariate::AbstractCovariate{T}, maxdepth::Integer, Î»::T, Î³::T,
                  minâˆ‚Â²ğ‘™::T, slicelength::Integer, singlethread::Bool) where {T<:AbstractFloat}

    len = length(âˆ‚ğ‘™covariate)
    maxnodecount = 2 ^ maxdepth
    nodeids = maxnodecount <= typemax(UInt8) ? ones(UInt8, len) : (maxnodecount <= typemax(UInt16) ? ones(UInt16, len) : ones(UInt32, len))
    intercept = ConstFactor(len)
    grad0 = sumgradient(nodeids, [true], intercept, [LevelPartition([true], false)], âˆ‚ğ‘™covariate, âˆ‚Â²ğ‘™covariate, slicelength, singlethread)[1][1]
    nodes0 = Vector{TreeNode{T}}()
    push!(nodes0, LeafNode{T}(grad0, true, Dict([f => LevelPartition(ones(Bool, length(getlevels(f))), true) for f in factors])))
    state0 = TreeGrowState{T}(nodeids, nodes0, factors, âˆ‚ğ‘™covariate, âˆ‚Â²ğ‘™covariate, Î», Î³, minâˆ‚Â²ğ‘™, slicelength, singlethread)
    layers = collect(Iterators.take(Seq(TreeLayer{T}, state0, nextlayer), maxdepth))
    tree = Tree{T}(layers, Î», Î³, minâˆ‚Â²ğ‘™, maxdepth, slicelength, singlethread)
    pred = predict(tree.layers[end], nodeids, Î»)
    tree, pred
end

